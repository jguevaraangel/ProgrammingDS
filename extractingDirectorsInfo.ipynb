{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "import re\n",
    "import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Awards from Directors URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_movies_info(name_archive: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function reads a file containing movie information and returns a dataframe\n",
    "    containing the movie information.\n",
    "\n",
    "    Args:\n",
    "        name_archive: name of the xlsx archive with the movie info\n",
    "\n",
    "    Returns:\n",
    "        movies_df: dataframe with the movie info\n",
    "    \"\"\"\n",
    "    movies_df = pd.read_csv(name_archive)\n",
    "    # Convert stringified lists to actual Python lists\n",
    "    movies_df['directors'] = movies_df['directors'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else []\n",
    "    )\n",
    "\n",
    "    # Now extract names and URLs\n",
    "    movies_df['director_names'] = movies_df['directors'].apply(\n",
    "        lambda x: ', '.join([d.get('name', '') for d in x]) if isinstance(x, list) else ''\n",
    "    )\n",
    "\n",
    "    movies_df['director_urls'] = movies_df['directors'].apply(\n",
    "        lambda x: ', '.join([d.get('url', '') for d in x]) if isinstance(x, list) else ''\n",
    "    )\n",
    "    return movies_df\n",
    "\n",
    "def parsing_directors_URL(movies_df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    This function receives the dataframe with the movie info and parse the\n",
    "    director urls and save it into a list.\n",
    "\n",
    "    Args:\n",
    "        movies_df: dataframe with the movie info\n",
    "\n",
    "    Returns:\n",
    "        director_urls: list with the directors urls\n",
    "    \"\"\"\n",
    "    # Get all non-empty director_urls, split by comma, and flatten\n",
    "    director_urls = movies_df[movies_df['director_urls'] != '']['director_urls'] \\\n",
    "        .str.split(', ') \\\n",
    "        .explode() \\\n",
    "        .tolist()\n",
    "    return director_urls\n",
    "\n",
    "def get_director_raw_info(url: str) -> tuple[list[Tag], str]:\n",
    "    \"\"\"\n",
    "    This function retrieves the raw HTML content of a director's IMDb page\n",
    "    and extracts the raw info block and the director's name.\n",
    "\n",
    "    Args:\n",
    "        url: IMDb URL of the director's page\n",
    "\n",
    "    Returns:\n",
    "        dir_info_raw: list of HTML elements containing the raw data about the director\n",
    "        name: name of the director\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    dir_info_raw = soup.find_all('li', {'data-testid': True})\n",
    "\n",
    "    # Extracting director name from the title\n",
    "    title_tag = soup.find('title')\n",
    "    name = title_tag.get_text(strip=True).split('-')[0].strip() if title_tag else 'Nombre no encontrado'\n",
    "\n",
    "    return dir_info_raw, name\n",
    "\n",
    "def get_director_structured_info(name: str, item: Tag, director_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    This function takes the raw HTML element of a director's credit and returns\n",
    "    a structured dictionary with relevant information.\n",
    "\n",
    "    Args:\n",
    "        name: name of the director\n",
    "        item: raw HTML item from the IMDb page\n",
    "\n",
    "    Returns:\n",
    "        response: structured information including name, category, movie name, rating, and URL;\n",
    "                  if an error occurs, a dictionary with an error message is returned\n",
    "    \"\"\"\n",
    "    try:\n",
    "        category = item.get(\"data-testid\", default=\"no_category\")\n",
    "\n",
    "        if not category.startswith(\"cred\"):\n",
    "            return {\"error\": \"unexpected category\"}\n",
    "\n",
    "        match = re.search(r'_(.*?)_', category)\n",
    "        category_cleaned = unidecode.unidecode(match.group(1))\n",
    "\n",
    "        info = item.find('a', {'aria-label': True})\n",
    "        movie_name = info[\"aria-label\"]\n",
    "        url = f\"https://www.imdb.com{info.get('href')}\"\n",
    "\n",
    "        rating_span = item.find('span', class_='ipc-rating-star--rating')\n",
    "        rating = rating_span.get_text(strip=True) if rating_span else 'N/A'\n",
    "\n",
    "        response = {\n",
    "            \"name\": name,\n",
    "            \"category\": category_cleaned,\n",
    "            \"movie_name\": movie_name,\n",
    "            \"rating\": rating,\n",
    "            \"movie_url\": url,\n",
    "            \"director_url\": director_url\n",
    "        }\n",
    "        return response\n",
    "    except Exception as ex:\n",
    "        return {\"error\": f\"{ex}\"}\n",
    "\n",
    "def obtaining_info_per_url(urls: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    This function loops through a list of director IMDb URLs, parses the raw and structured\n",
    "    information, and aggregates it into a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        urls: list of IMDb URLs for directors\n",
    "\n",
    "    Returns:\n",
    "        all_structured_data: list of dictionaries containing structured movie info for each director\n",
    "    \"\"\"\n",
    "    all_structured_data = []\n",
    "\n",
    "    for url in tqdm(urls):\n",
    "        try:\n",
    "            raw_info, director_name = get_director_raw_info(url)\n",
    "\n",
    "            structured_data = list(\n",
    "                filter(\n",
    "                    lambda item: \"error\" not in item,\n",
    "                    map(lambda item: get_director_structured_info(director_name, item, url), raw_info)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            all_structured_data.extend(structured_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")\n",
    "    return all_structured_data\n",
    "\n",
    "def scrapeDirectorAwards(director_url: str):\n",
    "    awards_url = director_url.rstrip('/') + \"/awards\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(awards_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to scrape {awards_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    award_divs = soup.select(\".ipc-metadata-list-summary-item__tc\")\n",
    "\n",
    "    # Extract director's name from the <title> tag or header\n",
    "    director_name = \"\"\n",
    "    try:\n",
    "        director_name_tag = soup.select_one(\"title\")\n",
    "        if director_name_tag:\n",
    "            director_name = director_name_tag.text.split(\"-\")[0].strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    results = []\n",
    "    for award_div in award_divs:\n",
    "        try:\n",
    "            main_award = award_div.select_one(\"a.ipc-metadata-list-summary-item__t\")\n",
    "            if not main_award:\n",
    "                continue\n",
    "            year_result = main_award.contents[0].strip()\n",
    "            award_name = main_award.select_one(\"span\").text.strip()\n",
    "\n",
    "            category_tag = award_div.select_one(\".awardCategoryName\")\n",
    "            category = category_tag.text.strip() if category_tag else \"\"\n",
    "\n",
    "            title_tag = award_div.select_one(\".ipc-metadata-list-summary-item__stl a\")\n",
    "            title = title_tag.text.strip() if title_tag else \"\"\n",
    "\n",
    "            results.append({\n",
    "                \"director_name\": director_name,\n",
    "                \"director_url\": director_url,\n",
    "                \"year_result\": year_result,\n",
    "                \"award_name\": award_name,\n",
    "                \"category\": category,\n",
    "                \"title\": title\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to parse one award block: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies_df = parsing_movies_info(\"outputs/movies.csv\")\n",
    "# urls = parsing_directors_URL(movies_df)\n",
    "# all_structured_data = obtaining_info_per_url(urls)\n",
    "# df_directors = pd.DataFrame(all_structured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_awards = pd.DataFrame()\n",
    "# for url in tqdm(urls):\n",
    "#     awards_data = scrapeDirectorAwards(url)\n",
    "#     if awards_data:  # Only proceed if something was scraped\n",
    "#         awards_df = pd.DataFrame(awards_data)\n",
    "#         all_awards = pd.concat([all_awards, awards_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_awards.to_csv(\"./outputs/directors_awards_info.csv\", index=False)\n",
    "# df_directors.to_csv(\"./outputs/directors_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting top directors from Awards pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oscars Awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.imdb.com\"\n",
    "OSCARS_HOME = f\"{BASE_URL}/es-es/oscars/\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_director_urls_from_section(section):\n",
    "    directors = []\n",
    "    # Find all <a> tags that contain \"/name/\" in href\n",
    "    director_links = section.select('a[href*=\"/name/\"]')\n",
    "\n",
    "    for a in director_links:\n",
    "        relative_url = a.get(\"href\")\n",
    "        full_url = BASE_URL + relative_url.split(\"?\")[0]\n",
    "        directors.append(full_url)\n",
    "    \n",
    "    return directors[0]\n",
    "\n",
    "def get_year_urls_by_decade_with_selenium():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"window-size=1920x1080\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(OSCARS_HOME)\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    decade_tabs = driver.find_elements(By.CSS_SELECTOR, '.ipc-tabs[role=\"tablist\"] li[role=\"tab\"]')\n",
    "    decade_tabs = decade_tabs[:5]\n",
    "\n",
    "    decade_to_years = {}\n",
    "\n",
    "    for i, tab in enumerate(decade_tabs):\n",
    "        decade_label = tab.text.strip()\n",
    "        print(f\"📆 Switching to: {decade_label}\")\n",
    "\n",
    "        try:\n",
    "            # Scroll into view and wait until clickable\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", tab)\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable(tab))\n",
    "            time.sleep(1)\n",
    "            tab.click()\n",
    "            time.sleep(2)  # Let the page update\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not click tab for {decade_label}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Parse page source after switching\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        year_links = soup.select(\".ipc-chip-list__scroller a[href*='/event/ev0000003/']\")\n",
    "        years = [(a.text.strip(), \"https://www.imdb.com\" + a['href']) for a in year_links][:-1]\n",
    "\n",
    "        decade_to_years[decade_label] = years\n",
    "        time.sleep(1)\n",
    "\n",
    "    driver.quit()\n",
    "    return decade_to_years\n",
    "\n",
    "def extract_best_director(year, url, decade):\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS)\n",
    "        res.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to get {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    section = soup.find(\"section\", {\"data-testid\": \"BestAchievementinDirecting\"})\n",
    "    if not section:\n",
    "        section = soup.find(\"section\", {\"data-testid\": \"BestDirector\"})\n",
    "        if not section:\n",
    "            print(f\"⚠️ No Best Director section found for {year}\")\n",
    "            return None\n",
    "        \n",
    "    nominees = []\n",
    "    nominee_items = section.select(\"li.ipc-metadata-list__item\")\n",
    "\n",
    "    for item in nominee_items:\n",
    "        try:\n",
    "            result_tag = item.select_one(\".ipc-signpost__text\")\n",
    "            result = result_tag.text.strip() if result_tag else \"Nominee\"\n",
    "\n",
    "            director_tag = item.select_one('a.ipc-link[href*=\"/name/\"]')\n",
    "            director = director_tag.text.strip() if director_tag else \"\"\n",
    "\n",
    "            director_url = extract_director_urls_from_section(item)\n",
    "\n",
    "            movie_tag = item.select_one('a[href*=\"/title/\"]')\n",
    "            movie = movie_tag.text.strip() if movie_tag else \"\"\n",
    "\n",
    "            nominees.append({\n",
    "                \"decade\": decade,\n",
    "                \"year\": year,\n",
    "                \"result\": result,\n",
    "                \"director\": director,\n",
    "                \"movie\": movie,\n",
    "                \"director url\": director_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error parsing nominee: {e}\")\n",
    "            continue\n",
    "    return nominees\n",
    "\n",
    "def scrape_all_best_director_winners():\n",
    "    decade_to_years = get_year_urls_by_decade_with_selenium()\n",
    "    all_data = []\n",
    "\n",
    "    for decade, year_list in decade_to_years.items():\n",
    "        for year, url in year_list:\n",
    "            print(f\"🔍 Scraping {year} from {url}\")\n",
    "            nominees = extract_best_director(year, url, decade)\n",
    "            all_data.extend(nominees)  # Append all nominees\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📆 Switching to: Década de 2020\n",
      "📆 Switching to: Década de 2010\n",
      "📆 Switching to: Década de 2000\n",
      "📆 Switching to: Década de 1990\n",
      "📆 Switching to: Década de 1980\n",
      "🔍 Scraping 2025 from https://www.imdb.com/es-es/event/ev0000003/2025/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 2024 from https://www.imdb.com/es-es/event/ev0000003/2024/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 2023 from https://www.imdb.com/es-es/event/ev0000003/2023/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 2022 from https://www.imdb.com/es-es/event/ev0000003/2022/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 2021 from https://www.imdb.com/es-es/event/ev0000003/2021/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 2020 from https://www.imdb.com/es-es/event/ev0000003/2020/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 2019 from https://www.imdb.com/es-es/event/ev0000003/2019/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 2018 from https://www.imdb.com/es-es/event/ev0000003/2018/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 2017 from https://www.imdb.com/es-es/event/ev0000003/2017/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 2016 from https://www.imdb.com/es-es/event/ev0000003/2016/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 2015 from https://www.imdb.com/es-es/event/ev0000003/2015/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 2014 from https://www.imdb.com/es-es/event/ev0000003/2014/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 2013 from https://www.imdb.com/es-es/event/ev0000003/2013/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_7\n",
      "🔍 Scraping 2012 from https://www.imdb.com/es-es/event/ev0000003/2012/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_8\n",
      "🔍 Scraping 2011 from https://www.imdb.com/es-es/event/ev0000003/2011/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_9\n",
      "🔍 Scraping 2010 from https://www.imdb.com/es-es/event/ev0000003/2010/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_10\n",
      "🔍 Scraping 2009 from https://www.imdb.com/es-es/event/ev0000003/2009/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 2008 from https://www.imdb.com/es-es/event/ev0000003/2008/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 2007 from https://www.imdb.com/es-es/event/ev0000003/2007/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 2006 from https://www.imdb.com/es-es/event/ev0000003/2006/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 2005 from https://www.imdb.com/es-es/event/ev0000003/2005/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 2004 from https://www.imdb.com/es-es/event/ev0000003/2004/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 2003 from https://www.imdb.com/es-es/event/ev0000003/2003/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_7\n",
      "🔍 Scraping 2002 from https://www.imdb.com/es-es/event/ev0000003/2002/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_8\n",
      "🔍 Scraping 2001 from https://www.imdb.com/es-es/event/ev0000003/2001/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_9\n",
      "🔍 Scraping 2000 from https://www.imdb.com/es-es/event/ev0000003/2000/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_10\n",
      "🔍 Scraping 1999 from https://www.imdb.com/es-es/event/ev0000003/1999/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 1998 from https://www.imdb.com/es-es/event/ev0000003/1998/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 1997 from https://www.imdb.com/es-es/event/ev0000003/1997/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 1996 from https://www.imdb.com/es-es/event/ev0000003/1996/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 1995 from https://www.imdb.com/es-es/event/ev0000003/1995/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 1994 from https://www.imdb.com/es-es/event/ev0000003/1994/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 1993 from https://www.imdb.com/es-es/event/ev0000003/1993/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_7\n",
      "🔍 Scraping 1992 from https://www.imdb.com/es-es/event/ev0000003/1992/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_8\n",
      "🔍 Scraping 1991 from https://www.imdb.com/es-es/event/ev0000003/1991/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_9\n",
      "🔍 Scraping 1990 from https://www.imdb.com/es-es/event/ev0000003/1990/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_10\n",
      "🔍 Scraping 1989 from https://www.imdb.com/es-es/event/ev0000003/1989/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 1988 from https://www.imdb.com/es-es/event/ev0000003/1988/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 1987 from https://www.imdb.com/es-es/event/ev0000003/1987/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 1986 from https://www.imdb.com/es-es/event/ev0000003/1986/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 1985 from https://www.imdb.com/es-es/event/ev0000003/1985/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 1984 from https://www.imdb.com/es-es/event/ev0000003/1984/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 1983 from https://www.imdb.com/es-es/event/ev0000003/1983/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_7\n",
      "🔍 Scraping 1982 from https://www.imdb.com/es-es/event/ev0000003/1982/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_8\n",
      "🔍 Scraping 1981 from https://www.imdb.com/es-es/event/ev0000003/1981/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_9\n",
      "🔍 Scraping 1980 from https://www.imdb.com/es-es/event/ev0000003/1980/1/?ref_=fea_acd_ww_fea_eds_center-29_yr_10\n"
     ]
    }
   ],
   "source": [
    "oscars_info = scrape_all_best_director_winners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars_info.to_csv('outputs/oscars_awards_directors_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [03:54<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "oscars_directors_url = oscars_info[\"director url\"].unique().tolist()\n",
    "oscars_directors_complete_info = obtaining_info_per_url(oscars_directors_url)\n",
    "df_oscars_directors_complete_info = pd.DataFrame(oscars_directors_complete_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oscars_directors_complete_info.to_csv('outputs/oscars_directors_complete_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden Globes Awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.imdb.com\"\n",
    "GG_HOME = f\"{BASE_URL}/es-es/golden-globes/\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_urls_by_decade_with_selenium():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"window-size=1920x1080\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(GG_HOME)\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    decade_tabs = driver.find_elements(By.CSS_SELECTOR, '.ipc-tabs[role=\"tablist\"] li[role=\"tab\"]')\n",
    "    decade_tabs = decade_tabs[:5]\n",
    "\n",
    "    decade_to_years = {}\n",
    "\n",
    "    for i, tab in enumerate(decade_tabs):\n",
    "        decade_label = tab.text.strip()\n",
    "        print(f\"📆 Switching to: {decade_label}\")\n",
    "\n",
    "        try:\n",
    "            # Scroll into view and wait until clickable\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", tab)\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable(tab))\n",
    "            time.sleep(1)\n",
    "            tab.click()\n",
    "            time.sleep(2)  # Let the page update\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not click tab for {decade_label}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Parse page source after switching\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        year_links = soup.select(\".ipc-chip-list__scroller a[href*='/event/ev0000292/']\")\n",
    "        years = [(a.text.strip(), \"https://www.imdb.com\" + a['href']) for a in year_links][:-1]\n",
    "\n",
    "        decade_to_years[decade_label] = years\n",
    "        time.sleep(1)\n",
    "\n",
    "    driver.quit()\n",
    "    return decade_to_years\n",
    "\n",
    "def extract_best_director(year, url, decade):\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS)\n",
    "        res.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to get {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    section = soup.find(\"section\", {\"data-testid\": \"BestDirector,MotionPicture\"})\n",
    "    if not section:\n",
    "        section = soup.find(\"section\", {\"data-testid\": \"BestDirector-MotionPicture\"})\n",
    "        if not section:\n",
    "            print(f\"⚠️ No Best Director section found for {year}\")\n",
    "            return None\n",
    "        \n",
    "    nominees = []\n",
    "    nominee_items = section.select(\"li.ipc-metadata-list__item\")\n",
    "\n",
    "    for item in nominee_items:\n",
    "        try:\n",
    "            result_tag = item.select_one(\".ipc-signpost__text\")\n",
    "            result = result_tag.text.strip() if result_tag else \"Nominee\"\n",
    "\n",
    "            director_tag = item.select_one('a.ipc-link[href*=\"/name/\"]')\n",
    "            director = director_tag.text.strip() if director_tag else \"\"\n",
    "\n",
    "            director_url = extract_director_urls_from_section(item)\n",
    "\n",
    "            movie_tag = item.select_one('a[href*=\"/title/\"]')\n",
    "            movie = movie_tag.text.strip() if movie_tag else \"\"\n",
    "\n",
    "            nominees.append({\n",
    "                \"decade\": decade,\n",
    "                \"year\": year,\n",
    "                \"result\": result,\n",
    "                \"director\": director,\n",
    "                \"movie\": movie,\n",
    "                \"director url\": director_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error parsing nominee: {e}\")\n",
    "            continue\n",
    "    return nominees\n",
    "\n",
    "def scrape_all_best_director_winners():\n",
    "    decade_to_years = get_year_urls_by_decade_with_selenium()\n",
    "    all_data = []\n",
    "\n",
    "    for decade, year_list in decade_to_years.items():\n",
    "        for year, url in year_list:\n",
    "            print(f\"🔍 Scraping {year} from {url}\")\n",
    "            nominees = extract_best_director(year, url, decade)\n",
    "            all_data.extend(nominees)  # Append all nominees\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📆 Switching to: Década de 2020\n",
      "📆 Switching to: Década de 2010\n",
      "📆 Switching to: Década de 2000\n",
      "📆 Switching to: Década de 1990\n",
      "📆 Switching to: Década de 1980\n",
      "🔍 Scraping 2025 from https://www.imdb.com/es-es/event/ev0000292/2025/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 2024 from https://www.imdb.com/es-es/event/ev0000292/2024/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 2023 from https://www.imdb.com/es-es/event/ev0000292/2023/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 2022 from https://www.imdb.com/es-es/event/ev0000292/2022/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 2021 from https://www.imdb.com/es-es/event/ev0000292/2021/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 2020 from https://www.imdb.com/es-es/event/ev0000292/2020/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 2019 from https://www.imdb.com/es-es/event/ev0000292/2019/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 2018 from https://www.imdb.com/es-es/event/ev0000292/2018/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 2017 from https://www.imdb.com/es-es/event/ev0000292/2017/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 2016 from https://www.imdb.com/es-es/event/ev0000292/2016/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 2015 from https://www.imdb.com/es-es/event/ev0000292/2015/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 2014 from https://www.imdb.com/es-es/event/ev0000292/2014/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 2013 from https://www.imdb.com/es-es/event/ev0000292/2013/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_7\n",
      "🔍 Scraping 2012 from https://www.imdb.com/es-es/event/ev0000292/2012/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_8\n",
      "🔍 Scraping 2011 from https://www.imdb.com/es-es/event/ev0000292/2011/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_9\n",
      "🔍 Scraping 2010 from https://www.imdb.com/es-es/event/ev0000292/2010/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_10\n",
      "🔍 Scraping 2009 from https://www.imdb.com/es-es/event/ev0000292/2009/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 2008 from https://www.imdb.com/es-es/event/ev0000292/2008/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 2007 from https://www.imdb.com/es-es/event/ev0000292/2007/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 2006 from https://www.imdb.com/es-es/event/ev0000292/2006/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 2005 from https://www.imdb.com/es-es/event/ev0000292/2005/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 2004 from https://www.imdb.com/es-es/event/ev0000292/2004/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 2003 from https://www.imdb.com/es-es/event/ev0000292/2003/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_7\n",
      "🔍 Scraping 2002 from https://www.imdb.com/es-es/event/ev0000292/2002/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_8\n",
      "🔍 Scraping 2001 from https://www.imdb.com/es-es/event/ev0000292/2001/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_9\n",
      "🔍 Scraping 2000 from https://www.imdb.com/es-es/event/ev0000292/2000/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_10\n",
      "🔍 Scraping 1999 from https://www.imdb.com/es-es/event/ev0000292/1999/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 1998 from https://www.imdb.com/es-es/event/ev0000292/1998/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 1997 from https://www.imdb.com/es-es/event/ev0000292/1997/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 1996 from https://www.imdb.com/es-es/event/ev0000292/1996/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 1995 from https://www.imdb.com/es-es/event/ev0000292/1995/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 1994 from https://www.imdb.com/es-es/event/ev0000292/1994/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 1993 from https://www.imdb.com/es-es/event/ev0000292/1993/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_7\n",
      "🔍 Scraping 1992 from https://www.imdb.com/es-es/event/ev0000292/1992/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_8\n",
      "🔍 Scraping 1991 from https://www.imdb.com/es-es/event/ev0000292/1991/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_9\n",
      "🔍 Scraping 1990 from https://www.imdb.com/es-es/event/ev0000292/1990/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_10\n",
      "🔍 Scraping 1989 from https://www.imdb.com/es-es/event/ev0000292/1989/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_1\n",
      "🔍 Scraping 1988 from https://www.imdb.com/es-es/event/ev0000292/1988/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_2\n",
      "🔍 Scraping 1987 from https://www.imdb.com/es-es/event/ev0000292/1987/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_3\n",
      "🔍 Scraping 1986 from https://www.imdb.com/es-es/event/ev0000292/1986/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_4\n",
      "🔍 Scraping 1985 from https://www.imdb.com/es-es/event/ev0000292/1985/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_5\n",
      "🔍 Scraping 1984 from https://www.imdb.com/es-es/event/ev0000292/1984/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_6\n",
      "🔍 Scraping 1983 from https://www.imdb.com/es-es/event/ev0000292/1983/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_7\n",
      "🔍 Scraping 1982 from https://www.imdb.com/es-es/event/ev0000292/1982/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_8\n",
      "🔍 Scraping 1981 from https://www.imdb.com/es-es/event/ev0000292/1981/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_9\n",
      "🔍 Scraping 1980 from https://www.imdb.com/es-es/event/ev0000292/1980/1/?ref_=fea_globes_ww_fea_eds_center-29_yr_10\n"
     ]
    }
   ],
   "source": [
    "golden_globes_info = scrape_all_best_director_winners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_globes_info.to_csv('outputs/golden_globes_awards_directors_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [04:39<00:00,  2.10s/it]\n"
     ]
    }
   ],
   "source": [
    "gg_directors_url = golden_globes_info[\"director url\"].unique().tolist()\n",
    "gg_directors_complete_info = obtaining_info_per_url(gg_directors_url)\n",
    "df_gg_directors_complete_info = pd.DataFrame(gg_directors_complete_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gg_directors_complete_info.to_csv('outputs/golden_globes_directors_complete_info.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
